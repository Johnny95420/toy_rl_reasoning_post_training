hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  gen_batch_size: ${data.train_batch_size}

actor_rollout_ref:  
  model:
    lora_rank: 64
    lora_alpha: 64
    # [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]
    target_modules: [q_proj, k_proj, v_proj, "o_proj" ,"gate_proj" ,"up_proj" ,"down_proj"]

reward_model:
  reward_manager: dapo
  # sequence over long penalty
  overlong_buffer:
    # We try to avoid forgetting to set enable 
    enable: True 
    len: 4096
    penalty_factor: 1.0
    log: False

algorithm:
  adv_estimator: grpo
  filter_groups:
    enable: True # We try to avoid forgetting to set enable
    metric: acc # acc / score / seq_reward / seq_final_reward / ...
    max_num_gen_batches: 0 # Non-positive values mean no upper limit

trainer:
  project_name: verl-dapo
  resume_actor: False
  resume_critic: False
  resume_data: True
